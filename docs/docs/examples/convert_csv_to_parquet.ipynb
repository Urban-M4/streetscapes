{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/clairedonnelly/Documents/Urban-M4/test/streetscapes/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import duckdb\n",
    "\n",
    "from streetscapes import conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert CSV files to parquet and merging them together\n",
    "\n",
    "The CSV files of the original Global Streetscapes dataset add up to 64GB in total. Moreover, data is split in several files which can make it a bit cumbersome to work with. Here, we convert the data to Parquet, which reduces file size and makes it easier to load and manipulate the data. \n",
    "\n",
    "Additionally, we combine columns from several sources into a single dataset that should serve most usecases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simplemaps\n",
      "perception\n",
      "osm\n",
      "places365\n",
      "segmentation\n",
      "contextual\n",
      "metadata_common_attributes\n",
      "ghsl\n"
     ]
    }
   ],
   "source": [
    "# Convert all csvs in data dir to parquet \n",
    "for file in (conf.DATA_DIR / \"data\").glob(\"*.csv\"):\n",
    "    print(file.stem)\n",
    "    duckdb.sql(f\"\"\"\n",
    "        COPY '{file}'\n",
    "        TO '{file.with_suffix(\".parquet\")}' \n",
    "        (FORMAT 'parquet', COMPRESSION 'zstd')\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human_readable(csv_size)='13.09 GB', human_readable(parquet_size)='3.39 GB'\n",
      "reduction_factor=3.861482728503803\n"
     ]
    }
   ],
   "source": [
    "csv_size = sum(file.stat().st_size for file in (conf.DATA_DIR / \"data\").glob(\"*.csv\") if file.is_file())\n",
    "parquet_size = sum(file.stat().st_size for file in (conf.DATA_DIR / \"data\").glob(\"*.parquet\") if file.is_file() and file.name not in [\"combined.parquet\", \"streetscapes.parquet\"])\n",
    "\n",
    "def human_readable(size):\n",
    "    \"\"\"Format byte size in human readable format\"\"\"\n",
    "    order_of_magnitude = size.bit_length() // 10  # Dividing by 10 for base-1024 magnitude\n",
    "    match order_of_magnitude:\n",
    "        case 3:\n",
    "            return f\"{size / 1024**3:.2f} GB\"\n",
    "        case 2:\n",
    "            return f\"{size / 1024**2:.2f} MB\"\n",
    "        case 1:\n",
    "            return f\"{size / 1024:.2f} KB\"\n",
    "        case _:\n",
    "            return f\"{size} bytes\"\n",
    "\n",
    "print(f\"{human_readable(csv_size)=}, {human_readable(parquet_size)=}\")\n",
    "\n",
    "reduction_factor = csv_size/parquet_size\n",
    "print(f\"{reduction_factor=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We may want to combine multiple csv files together into a single parquet file. If we use JOIN like above on the full table, we quickly run into memory issues. This is because `duckdb.sql(...)` creates an in-memory database to load the data and keep track of intermediate results. Alternatively, duckdb can create a persistent database on disk using `duckdb.connect('database_filename')`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "CatalogException",
     "evalue": "Catalog Error: Table with name \"contextual\" already exists!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCatalogException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m files \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontextual\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata_common_attributes\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mosm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     ]\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mduckdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mduck.db\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcon\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Load each dataset onto disk from the each file\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCREATE TABLE \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfilename\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m AS SELECT * FROM \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDATA_DIR\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/data/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfilename\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m duckdb\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mduck.db\u001b[39m\u001b[38;5;124m\"\u001b[39m, config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthreads\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m}) \u001b[38;5;28;01mas\u001b[39;00m con:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Load each dataset onto disk from the each file\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[0;32m---> 15\u001b[0m         \u001b[43mcon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCREATE TABLE \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfilename\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m AS SELECT * FROM \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDATA_DIR\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/data/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfilename\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Perform the joins.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, filename \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(files[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;66;03m# Join the tables one by one and store intermediate results in separate tables\u001b[39;00m\n",
      "\u001b[0;31mCatalogException\u001b[0m: Catalog Error: Table with name \"contextual\" already exists!"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "        \"contextual\",\n",
    "        \"metadata_common_attributes\",\n",
    "        \"segmentation\",\n",
    "        \"simplemaps\",\n",
    "        \"ghsl\",\n",
    "        \"perception\",\n",
    "        \"places365\",\n",
    "        \"osm\",\n",
    "    ]\n",
    "\n",
    "with duckdb.connect(\"duck.db\", config={'threads': 1}) as con:\n",
    "    # Load each dataset onto disk from the each file\n",
    "    for filename in files:\n",
    "        con.sql(f\"CREATE TABLE {filename} AS SELECT * FROM '{conf.DATA_DIR}/data/{filename}.parquet'\")\n",
    "    \n",
    "    # Perform the joins.\n",
    "    for i, filename in enumerate(files[:-1]):\n",
    "        # Join the tables one by one and store intermediate results in separate tables\n",
    "        j = i + 1\n",
    "        target = filename if i==0 else f\"step{i}\"\n",
    "        con.sql(f\"CREATE TABLE step{j} AS SELECT * FROM {target} JOIN {files[j]} USING (UUID, source, orig_id)\")     \n",
    "    \n",
    "    # Finally, we can export the joined table to a new parquet file\n",
    "    con.sql(f\"COPY step{j} TO '{conf.DATA_DIR}/data/streetscapes.parquet' (FORMAT 'parquet', COMPRESSION 'zstd')\")\n",
    "\n",
    "# Remove the database from our filesystem\n",
    "os.remove(\"duck.db\")\n",
    "\n",
    "# Show the combined file size:\n",
    "combined_size = (conf.DATA_DIR / \"data\" / \"streetscapes.parquet\").stat().st_size\n",
    "print(f\"{human_readable(combined_size)=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's inspect the new file to see if the join has worked\n",
    "duckdb.sql(f\"SELECT * FROM '{conf.DATA_DIR}/data/streetscapes.parquet'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some usecases it might be more convenient to select certain columns from different files into a single table. This can be achieved in a similar manner to the previous example. Here, we create a dictionary with the file names and columns we want to select. We also need to specify a column that is common to all files to join on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary choosing files and columns\n",
    "selection = {\n",
    "    \"contextual\": ['UUID', 'source', 'orig_id'],\n",
    "    \"osm\": ['UUID', 'road_width', 'type_highway'],\n",
    "    \"simplemaps\": ['UUID', 'city'],\n",
    "    \"metadata_common_attributes\": ['UUID', 'lat', 'lon']\n",
    "}\n",
    "\n",
    "with duckdb.connect(\"duck.db\", config={'threads': 1}) as con:\n",
    "    # Load each dataset onto disk from the each file\n",
    "    for file, columns in selection.items():\n",
    "        col_str = ', '.join(columns) \n",
    "        con.sql(f\"CREATE TABLE {file} AS SELECT {col_str} FROM '{conf.DATA_DIR}/data/{file}.parquet'\")\n",
    "\n",
    "    # Perform the joins.\n",
    "    items = list(selection.items())\n",
    "    for i, (file, columns) in enumerate(items[:-1]):\n",
    "        # Join the tables one by one and store intermediate results in separate tables\n",
    "        j = i + 1\n",
    "        target = file if i==0 else f\"step{i}\"\n",
    "        next_file = items[j][0]\n",
    "        con.sql(f\"CREATE TABLE step{j} AS SELECT * FROM {target} JOIN {next_file} USING (UUID)\")     \n",
    "\n",
    "    # Finally, we can export the joined table to a new parquet file\n",
    "    con.sql(f\"COPY step{i} TO '{conf.DATA_DIR}/data/combined.parquet' (FORMAT 'parquet', COMPRESSION 'zstd')\")\n",
    "\n",
    "os.remove(\"duck.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's inspect the new file to see if the join has worked\n",
    "duckdb.sql(f\"SELECT * FROM '{conf.DATA_DIR}/data/combined.parquet'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are in touch with the developers of the original Open Streetscapes dataset to add these parquet files to the dataset on huggingface."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
