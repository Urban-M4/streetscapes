{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/clairedonnelly/Documents/Urban-M4/test/streetscapes/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import duckdb\n",
    "\n",
    "from streetscapes import conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert CSV files to parquet and merging them together\n",
    "\n",
    "The CSV files of the original Global Streetscapes dataset add up to 64GB in total. Moreover, data is split in several files which can make it a bit cumbersome to work with. Here, we convert the data to Parquet, which reduces file size and makes it easier to load and manipulate the data. \n",
    "\n",
    "Additionally, we combine columns from several sources into a single dataset that should serve most usecases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/clairedonnelly/Documents/Urban-M4/streetscapes-data/data/simplemaps.csv\n",
      "/Users/clairedonnelly/Documents/Urban-M4/streetscapes-data/data/perception.csv\n",
      "/Users/clairedonnelly/Documents/Urban-M4/streetscapes-data/data/osm.csv\n",
      "/Users/clairedonnelly/Documents/Urban-M4/streetscapes-data/data/places365.csv\n",
      "/Users/clairedonnelly/Documents/Urban-M4/streetscapes-data/data/segmentation.csv\n",
      "/Users/clairedonnelly/Documents/Urban-M4/streetscapes-data/data/contextual.csv\n",
      "/Users/clairedonnelly/Documents/Urban-M4/streetscapes-data/data/metadata_common_attributes.csv\n",
      "/Users/clairedonnelly/Documents/Urban-M4/streetscapes-data/data/ghsl.csv\n"
     ]
    }
   ],
   "source": [
    "# Convert all csvs in data dir to parquet \n",
    "for file in (conf.DATA_DIR / \"data\").glob(\"*.csv\"):\n",
    "    print(file)\n",
    "    duckdb.sql(f\"\"\"\n",
    "        COPY '{file}'\n",
    "        TO '{file.with_suffix(\".parquet\")}' \n",
    "        (FORMAT 'parquet', COMPRESSION 'zstd')\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human_readable(csv_size)='13.09 GB', human_readable(parquet_size)='3.39 GB'\n",
      "reduction_factor=3.8614643432586115\n"
     ]
    }
   ],
   "source": [
    "csv_size = sum(file.stat().st_size for file in (conf.DATA_DIR / \"data\").glob(\"*.csv\") if file.is_file())\n",
    "parquet_size = sum(file.stat().st_size for file in (conf.DATA_DIR / \"data\").glob(\"*.parquet\") if file.is_file() and not file.name==\"combined.parquet\")\n",
    "\n",
    "def human_readable(size):\n",
    "    \"\"\"Format byte size in human readable format\"\"\"\n",
    "    order_of_magnitude = size.bit_length() // 10  # Dividing by 10 for base-1024 magnitude\n",
    "    match order_of_magnitude:\n",
    "        case 3:\n",
    "            return f\"{size / 1024**3:.2f} GB\"\n",
    "        case 2:\n",
    "            return f\"{size / 1024**2:.2f} MB\"\n",
    "        case 1:\n",
    "            return f\"{size / 1024:.2f} KB\"\n",
    "        case _:\n",
    "            return f\"{size} bytes\"\n",
    "\n",
    "print(f\"{human_readable(csv_size)=}, {human_readable(parquet_size)=}\")\n",
    "\n",
    "reduction_factor = csv_size/parquet_size\n",
    "print(f\"{reduction_factor=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some usecases it might be more convenient to combine several (if not all) columns in a single table. If we use JOIN like above on the full table, we quickly run into memory issues. This is because `duckdb.sql(...)` creates an in-memory database to load the data and keep track of intermediate results. Alternatively, duckdb can create a persistent database on disk using `duckdb.connect('database_filename')`. Here, we combine all csvs into a single parquet file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contextual\n",
      "metadata_common_attributes\n",
      "segmentation\n",
      "simplemaps\n",
      "ghsl\n",
      "perception\n",
      "places365\n",
      "human_readable(combined_size)='0.53 GB'\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "        \"contextual\",\n",
    "        \"metadata_common_attributes\",\n",
    "        \"segmentation\",\n",
    "        \"simplemaps\",\n",
    "        \"ghsl\",\n",
    "        \"perception\",\n",
    "        \"places365\",\n",
    "        \"osm\",\n",
    "    ]\n",
    "\n",
    "with duckdb.connect(\"duck.db\", config={'threads': 1}) as con:\n",
    "    for filename in files:\n",
    "        con.sql(f\"CREATE TABLE {filename} AS SELECT * FROM '{conf.DATA_DIR}/data/{filename}.parquet'\")\n",
    "    \n",
    "    # Now we can perform the joins. We will do it step by step to keep checks on the memory needed to perform each join.\n",
    "    for i, filename in enumerate(files[:-1]):\n",
    "        # First we need to copy the content of each csv file into the database\n",
    "        j = i + 1  \n",
    "        con.sql(f\"CREATE TABLE step{j} AS SELECT * FROM {filename} JOIN {files[j]} USING (UUID, source, orig_id)\")     \n",
    "        print(filename)\n",
    "    \n",
    "    # Finally, we can export the joined table to a new parquet file\n",
    "    con.sql(f\"COPY step7 TO '{conf.DATA_DIR}/data/combined.parquet' (FORMAT 'parquet', COMPRESSION 'zstd')\")\n",
    "\n",
    "# Remove the database from our filesystem\n",
    "os.remove(\"duck.db\")\n",
    "\n",
    "# Show the combined file size:\n",
    "combined_size = (conf.DATA_DIR / \"data\" / \"combined.parquet\").stat().st_size\n",
    "print(f\"{human_readable(combined_size)=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's inspect the new file to see if the join has worked\n",
    "duckdb.sql(f\"SELECT * FROM '{conf.DATA_DIR}/data/combined.parquet'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# - Only keep columns that we think are relevant\n",
    "# - Could join on UUID only by either dropping source and orig_id of all but the first dataset, or by only selecting columns of interest for each dataset\n",
    "# - Could delete intermediate tables (e.g. step1 could be removed once step2 has been created)\n",
    "#\n",
    "# # Perhaps something like \n",
    "# selection = {\n",
    "#     \"contextual\": ['source', 'orig_id'],\n",
    "#     \"osm.csv\": ['road_width', 'type_highway'],\n",
    "#     \"simplemaps.csv\": ['city'],\n",
    "#     \"metadata_common_attributes.csv\": ['lat', 'lon']\n",
    "# }\n",
    "# for file, columns in selection:\n",
    "#     con.sql(f\"CREATE TABLE contextual AS SELECT {columns} FROM '{conf.DATA_DIR}/data/{file}.parquet'\")\n",
    "\n",
    "# for i, file in enumerate(selection.keys):\n",
    "#     con.sql(\"CREATE TABLE step{i} AS SELECT * FROM contextual JOIN metadata USING (UUID, source, orig_id)\")\n",
    "\n",
    "# con.sql(f\"COPY step{i} TO '{conf.DATA_DIR}/data/combined.parquet' (FORMAT 'parquet', COMPRESSION 'zstd')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are in touch with the developers of the original Open Streetscapes dataset to add these parquet files to the dataset on huggingface."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
